"""
YouTube Trends & Translation Module
"""
import os
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from collections import Counter
import re
from pytrends.request import TrendReq
from deep_translator import GoogleTranslator
import deepl
from dotenv import load_dotenv

# Import YouTube API
from . import youtube_api

load_dotenv()

# YouTube Category IDs (official YouTube category mapping)
YOUTUBE_CATEGORIES = {
    "ê²Œì„": 20,
    "ê³¼í•™/ê¸°ìˆ ": 28,
    "êµìœ¡": 27,
    "ë…¸í•˜ìš°/ìŠ¤íƒ€ì¼": 26,
    "ë‰´ìŠ¤/ì •ì¹˜": 25,
    "ë¹„ì˜ë¦¬/ì‚¬íšŒìš´ë™": 29,
    "ìŠ¤í¬ì¸ ": 17,
    "ì• ì™„ë™ë¬¼/ë™ë¬¼": 15,
    "ì—”í„°í…Œì¸ë¨¼íŠ¸": 24,
    "ì—¬í–‰/ì´ë²¤íŠ¸": 19,
    "ì˜í™”/ì• ë‹ˆë©”ì´ì…˜": 1,
    "ìŒì•…": 10
}

# Language codes for translation (Google Translate codes)
LANGUAGES = {
    "í•œêµ­ì–´": "ko",
    "ì˜ì–´": "en",
    "ì¼ë³¸ì–´": "ja",
    "ì¤‘êµ­ì–´": "zh-CN",  # Chinese Simplified
    "ìŠ¤í˜ì¸ì–´": "es",
    "íŒë””ì–´": "hi",
    "ëŸ¬ì‹œì•„ì–´": "ru"
}

# DeepL language codes (different from Google)
DEEPL_LANGUAGES = {
    "í•œêµ­ì–´": "KO",
    "ì˜ì–´": "EN-US",
    "ì¼ë³¸ì–´": "JA",
    "ì¤‘êµ­ì–´": "ZH",  # DeepL uses ZH for Chinese
    "ìŠ¤í˜ì¸ì–´": "ES",
    "íŒë””ì–´": "HI",
    "ëŸ¬ì‹œì•„ì–´": "RU"
}


class TrendsError(Exception):
    """Trends API error"""
    pass


class TranslationManager:
    """Manages translation using DeepL (if API key available) or Google Translate (free)"""

    def __init__(self):
        self.deepl_api_key = os.getenv("DEEPL_API_KEY", "")
        self.use_deepl = bool(self.deepl_api_key)

        if self.use_deepl:
            try:
                self.deepl_translator = deepl.Translator(self.deepl_api_key)
                print("âœ… DeepL API í™œì„±í™” (ê³ í’ˆì§ˆ ë²ˆì—­)")
            except Exception as e:
                print(f"âš ï¸  DeepL API ì˜¤ë¥˜: {e}")
                print("ğŸ“ Google Translateë¡œ ì „í™˜ (ë¬´ë£Œ)")
                self.use_deepl = False

    def translate(self, text: str, target_lang_name: str, source_lang: str = "ko") -> str:
        """
        Translate text to target language

        Args:
            text: Text to translate
            target_lang_name: Target language name (e.g., 'ì˜ì–´', 'ì¤‘êµ­ì–´')
            source_lang: Source language code (default: 'ko')

        Returns:
            Translated text
        """
        # Get target language code
        target_lang = LANGUAGES.get(target_lang_name, "en")

        # Skip if target is same as source
        if target_lang == source_lang or target_lang_name == "í•œêµ­ì–´":
            return text

        try:
            if self.use_deepl:
                return self._translate_deepl(text, target_lang_name, source_lang)
            else:
                return self._translate_google(text, target_lang, source_lang)
        except Exception as e:
            print(f"âš ï¸  ë²ˆì—­ ì‹¤íŒ¨ ({text[:20]}... â†’ {target_lang_name}): {e}")
            return text

    def _translate_deepl(self, text: str, target_lang_name: str, source_lang: str) -> str:
        """Translate using DeepL API"""
        # Clean text before translation
        cleaned_text = self._clean_for_translation(text)

        # If cleaned text is too short, return original
        if len(cleaned_text) < 2:
            return text

        try:
            # Convert to DeepL language codes
            target_deepl = DEEPL_LANGUAGES.get(target_lang_name, "EN-US")
            source_deepl = source_lang.upper()

            result = self.deepl_translator.translate_text(
                cleaned_text,
                source_lang=source_deepl,
                target_lang=target_deepl
            )

            if result.text:
                # Shorten the translation to keep it concise
                shortened = self._shorten_translation(result.text, max_words=3, max_chars=20)
                return shortened if shortened else result.text
            else:
                return text
        except Exception as e:
            print(f"âš ï¸  DeepL ë²ˆì—­ ì˜¤ë¥˜ ({text[:30]}...): {str(e)[:50]}")
            return text

    def _clean_for_translation(self, text: str) -> str:
        """Clean text for translation by removing emojis and special characters"""
        import re

        # Remove all emojis and special symbols
        cleaned = re.sub(r'[ğŸ®ğŸ¯ğŸ”¥ğŸ’¯ğŸ‘â¤ï¸ğŸ˜ŠğŸ˜‚ğŸ¤£ğŸ˜­ğŸ¥°ğŸ˜ğŸ¤”ğŸ’ªğŸ‰ğŸŠâœ¨â­ğŸŒŸğŸ’–ğŸ’•ğŸ’—ğŸ’ğŸ’˜ğŸ’“ğŸ’ğŸ’Ÿâ˜€ï¸ğŸŒ™â›…ğŸŒˆğŸµğŸ¶ğŸ¤ğŸ§ğŸ¬ğŸ“ºğŸ“·ğŸ“¸ğŸ¨ğŸ–¼ï¸âš¡ğŸ’¥ğŸ†ğŸ¥‡ğŸ¥ˆğŸ¥‰ğŸğŸ€]+', '', text)
        cleaned = re.sub(r'[â˜…â˜†â™¡â™¥â†’â†â†‘â†“â– â–¡â—â—‹â—†â—‡â–²â–³â–¼â–½â€»]+', '', cleaned)

        # Remove brackets and parentheses with content
        cleaned = re.sub(r'\[.*?\]', '', cleaned)
        cleaned = re.sub(r'\(.*?\)', '', cleaned)

        # Remove multiple spaces
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()

        # Remove quotes
        cleaned = cleaned.replace('"', '').replace("'", '')

        return cleaned

    def _shorten_translation(self, translated_text: str, max_words: int = 3, max_chars: int = 20) -> str:
        """
        Shorten translated text to keep it concise

        Args:
            translated_text: Translated text
            max_words: Maximum number of words (default: 3)
            max_chars: Maximum characters (default: 20)

        Returns:
            Shortened translation
        """
        import re

        # Remove articles and possessives
        text = translated_text
        text = re.sub(r"\b(the|The|a|A|an|An)\b\s*", "", text)  # English articles
        text = re.sub(r"'s\b", "", text)  # Possessive 's
        text = re.sub(r"\bã®\b", "", text)  # Japanese ã® (possessive)
        text = re.sub(r"\bçš„\b", "", text)  # Chinese çš„ (possessive)

        # Split into words
        words = text.split()

        # Take first max_words
        if len(words) > max_words:
            words = words[:max_words]

        result = ' '.join(words)

        # If still too long, truncate by characters
        if len(result) > max_chars:
            result = result[:max_chars].rsplit(' ', 1)[0]  # Cut at last word boundary

        return result.strip()

    def _translate_google(self, text: str, target_lang: str, source_lang: str) -> str:
        """Translate using Google Translate (free) with retry logic"""
        import time

        # Clean text before translation
        cleaned_text = self._clean_for_translation(text)

        # If cleaned text is too short, return original
        if len(cleaned_text) < 2:
            return text

        # Try translation with retry logic (max 3 attempts)
        max_retries = 3
        for attempt in range(max_retries):
            try:
                translator = GoogleTranslator(source=source_lang, target=target_lang)
                result = translator.translate(cleaned_text)

                if result and len(result) > 0:
                    # Shorten the translation to keep it concise
                    shortened = self._shorten_translation(result, max_words=3, max_chars=20)
                    return shortened if shortened else result
                else:
                    # If no result, wait and retry
                    if attempt < max_retries - 1:
                        time.sleep(0.5 * (attempt + 1))  # Exponential backoff
                        continue
                    else:
                        return text

            except Exception as e:
                if attempt < max_retries - 1:
                    # Retry with delay
                    time.sleep(0.5 * (attempt + 1))
                    continue
                else:
                    # Final attempt failed, return original
                    print(f"âš ï¸  Google ë²ˆì—­ ì˜¤ë¥˜ ({text[:30]}...): {str(e)[:50]}")
                    return text

        return text

    def translate_to_all_languages(self, text: str, source_lang: str = "ko") -> Dict[str, str]:
        """
        Translate text to all supported languages

        Returns:
            Dictionary mapping language names to translated text
        """
        import time

        translations = {}

        for lang_name in LANGUAGES.keys():
            # í•œêµ­ì–´ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ
            if lang_name == "í•œêµ­ì–´":
                translations[lang_name] = text
            else:
                translations[lang_name] = self.translate(text, lang_name, source_lang)
                # Small delay to avoid rate limiting (only for Google Translate)
                if not self.use_deepl:
                    time.sleep(0.1)

        return translations


class TrendsExplorer:
    """Explore YouTube trending keywords by category"""

    def __init__(self):
        self.pytrends = TrendReq(hl='ko-KR', tz=540)  # Korea timezone
        self.translator = TranslationManager()

    def _extract_keywords_from_titles(self, titles: List[str], num_keywords: int = 20) -> List[str]:
        """
        Extract trending keyword phrases from video titles
        Focus on EXTREMELY short 2-word phrases ONLY

        Args:
            titles: List of video titles
            num_keywords: Number of keywords to extract

        Returns:
            List of extracted keyword phrases (EXACTLY 2 words, 4-12 characters)
        """
        keywords = []
        seen = set()

        # Korean particles (ì¡°ì‚¬) to remove from word endings
        particles = ['ì˜', 'ë¥¼', 'ì„', 'ì´', 'ê°€', 'ì—', 'ë„', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 'ì—ì„œ', 'ë¶€í„°', 'ê¹Œì§€']

        # Korean stopwords to remove
        stopwords = {
            'í•˜ëŠ”', 'ë˜ëŠ”', 'í•œ', 'ëœ', 'ìˆëŠ”', 'ì—†ëŠ”', 'ìœ„í•œ', 'ëŒ€í•œ', 'ìœ„í•´', 'ëŒ€í•´',
            'ìˆë‹¤', 'ì—†ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ì²œì²œíˆ', 'ë¹ ë¥´ê²Œ', 'ì •ë§', 'ì§„ì§œ',
            'ì™„ì „', 'ë„ˆë¬´', 'ì•„ì£¼', 'ë§¤ìš°', 'ë´ì•¼', 'ë³´ë©´', 'ë³´ì´ëŠ”', 'ë³´ëŠ”', 'ë³¸', 'ë³´ê³ ',
            'ì¤‘', 'ì†', 'ì•ˆ', 'ë°–', 'ìœ„', 'ì•„ë˜', 'ì•', 'ë’¤'
        }

        def remove_particles(word):
            """Remove Korean particles from word endings"""
            for particle in particles:
                if word.endswith(particle) and len(word) > len(particle) + 1:
                    return word[:-len(particle)]
            return word

        # Clean and prepare titles
        cleaned_titles = []
        for title in titles:
            # Remove emojis and special characters
            cleaned = re.sub(r'[ğŸ®ğŸ¯ğŸ”¥ğŸ’¯ğŸ‘â¤ï¸ğŸ˜ŠğŸ˜‚ğŸ¤£ğŸ˜­ğŸ¥°ğŸ˜ğŸ¤”ğŸ’ªğŸ‰ğŸŠâœ¨â­ğŸŒŸğŸ’–ğŸ’•ğŸ’—ğŸ’ğŸ’˜ğŸ’“ğŸ’ğŸ’Ÿâ˜€ï¸ğŸŒ™â›…ğŸŒˆğŸµğŸ¶ğŸ¤ğŸ§ğŸ¬ğŸ“ºğŸ“·ğŸ“¸ğŸ¨ğŸ–¼ï¸âš¡ğŸ’¥ğŸ†ğŸ¥‡ğŸ¥ˆğŸ¥‰ğŸğŸ€]+', '', title)
            cleaned = re.sub(r'[â˜…â˜†â™¡â™¥â†’â†â†‘â†“â– â–¡â—â—‹â—†â—‡â–²â–³â–¼â–½â€»]+', '', cleaned)

            # Remove hashtags (everything after #)
            if '#' in cleaned:
                cleaned = cleaned.split('#')[0]

            # Remove brackets and parentheses with content
            cleaned = re.sub(r'\[.*?\]', '', cleaned)
            cleaned = re.sub(r'\(.*?\)', '', cleaned)

            # Remove quotes and exclamation marks
            cleaned = re.sub(r'["""\'\'!?]+', '', cleaned)

            # Remove numbers and uppercase letters like "4M", "TRUE"
            cleaned = re.sub(r'\b[A-Z0-9]+\b', '', cleaned)
            cleaned = re.sub(r'\b\d+\w*\b', '', cleaned)

            # Remove multiple spaces
            cleaned = re.sub(r'\s+', ' ', cleaned).strip()

            if len(cleaned) >= 5:
                cleaned_titles.append(cleaned)

        # STRATEGY 1: Extract core 2-word phrases ONLY (frequency-based)
        phrase_counter = Counter()

        for title in cleaned_titles:
            # Split and remove particles
            words = []
            for w in title.split():
                if w not in stopwords and len(w) > 1:
                    clean_word = remove_particles(w)
                    if clean_word and len(clean_word) > 1:
                        words.append(clean_word)

            # Extract ONLY 2-word phrases (no 3-word!)
            for i in range(len(words) - 1):
                phrase = ' '.join(words[i:i + 2])

                # Filter by length (4-12 characters - very short!)
                if 4 <= len(phrase) <= 12:
                    # Skip if mostly numbers
                    if sum(c.isdigit() for c in phrase) < len(phrase) * 0.3:
                        phrase_counter[phrase] += 1

        # Add most common 2-word phrases ONLY
        for phrase, count in phrase_counter.most_common(num_keywords * 2):
            phrase_lower = phrase.lower()
            # Check word count (must be EXACTLY 2 words)
            word_count = len(phrase.split())
            if word_count == 2:
                if not any(phrase_lower in s or s in phrase_lower for s in seen):
                    keywords.append(phrase)
                    seen.add(phrase_lower)
                    if len(keywords) >= num_keywords:
                        return keywords

        # STRATEGY 2: Extract 2-word phrases from title beginnings
        for title in cleaned_titles:
            words = []
            for w in title.split():
                if w not in stopwords and len(w) > 1:
                    clean_word = remove_particles(w)
                    if clean_word and len(clean_word) > 1:
                        words.append(clean_word)

            # ONLY 2-word phrases from beginning
            if len(words) >= 2:
                phrase = ' '.join(words[:2])
                phrase_lower = phrase.lower()

                # Check length (4-12 characters)
                if 4 <= len(phrase) <= 12:
                    if not any(phrase_lower in s or s in phrase_lower for s in seen):
                        keywords.append(phrase)
                        seen.add(phrase_lower)
                        if len(keywords) >= num_keywords:
                            return keywords

        # STRATEGY 3: Single meaningful words as last resort
        for title in cleaned_titles:
            words = []
            for w in title.split():
                if w not in stopwords and len(w) > 2:
                    clean_word = remove_particles(w)
                    if clean_word and len(clean_word) > 2:
                        words.append(clean_word)

            for word in words[:3]:  # Take first 3 meaningful words
                if 3 <= len(word) <= 8:  # Single words: 3-8 characters
                    word_lower = word.lower()
                    if not any(word_lower in s or s in word_lower for s in seen):
                        keywords.append(word)
                        seen.add(word_lower)
                        if len(keywords) >= num_keywords:
                            return keywords

        # Calculate average word count
        avg_words = sum(len(k.split()) for k in keywords) / len(keywords) if keywords else 0
        avg_len = sum(len(k) for k in keywords) / len(keywords) if keywords else 0
        print(f"âœ… ì¶”ì¶œëœ êµ¬ë¬¸: {len(keywords)}ê°œ (í‰ê·  ë‹¨ì–´ ìˆ˜: {avg_words:.1f}ê°œ, í‰ê·  ê¸¸ì´: {avg_len:.1f}ì)")

        return keywords[:num_keywords]

    def get_trending_keywords(
        self,
        category: str,
        num_keywords: int = 20,
        timeframe: str = 'now 7-d'
    ) -> List[str]:
        """
        Get trending keywords for a YouTube category by analyzing popular videos

        Args:
            category: Category name (e.g., 'ê²Œì„', 'ìŠ¤í¬ì¸ ')
            num_keywords: Number of keywords to return
            timeframe: Time range (not used with YouTube API)

        Returns:
            List of trending keywords
        """
        if category not in YOUTUBE_CATEGORIES:
            raise TrendsError(f"Unknown category: {category}")

        category_id = YOUTUBE_CATEGORIES[category]

        try:
            # Get popular videos from YouTube API
            print(f"ğŸ“Š '{category}' ì¹´í…Œê³ ë¦¬ì˜ ì¸ê¸° ì˜ìƒ ì œëª© ë¶„ì„ ì¤‘...")

            # Fetch popular videos for this category
            video_ids = youtube_api.get_popular_videos_by_category(
                category_id,
                max_results=100,  # Analyze top 100 videos
                region_code='KR'  # Korea region
            )

            if not video_ids:
                print(f"âš ï¸  ì¸ê¸° ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²• ì‚¬ìš©...")
                return self._generate_category_keywords(category, num_keywords)

            # Get video details (titles)
            videos = youtube_api.get_videos_info(video_ids)

            if not videos:
                print(f"âš ï¸  ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²• ì‚¬ìš©...")
                return self._generate_category_keywords(category, num_keywords)

            # Extract titles
            titles = [video['title'] for video in videos if video.get('title')]

            print(f"âœ… {len(titles)}ê°œ ì˜ìƒ ì œëª© ìˆ˜ì§‘ ì™„ë£Œ")

            # Extract keywords from titles
            keywords = self._extract_keywords_from_titles(titles, num_keywords)

            if len(keywords) < 5:
                print(f"âš ï¸  ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ëŒ€ì²´ ë°©ë²• ì‚¬ìš©...")
                return self._generate_category_keywords(category, num_keywords)

            print(f"âœ… {len(keywords)}ê°œ íŠ¸ë Œë”© í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
            return keywords

        except Exception as e:
            print(f"âš ï¸  íŠ¸ë Œë“œ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            # Fallback: generate keywords
            return self._generate_category_keywords(category, num_keywords)

    def _generate_category_keywords(self, category: str, num_keywords: int) -> List[str]:
        """
        Generate keywords for a category using YouTube search (fallback)
        Uses search API to find real trending content instead of templates
        """
        print(f"ğŸ” ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•˜ì—¬ '{category}' ê´€ë ¨ ì¸ê¸° ì½˜í…ì¸ ë¥¼ ì°¾ëŠ” ì¤‘...")

        try:
            # Search for videos related to this category
            search_results = youtube_api.search_videos(
                query=category,
                max_results=50,
                order="viewCount"  # Order by view count for popular content
            )

            if search_results:
                # Get video IDs
                video_ids = [video['video_id'] for video in search_results]

                # Get detailed info
                videos = youtube_api.get_videos_info(video_ids)

                if videos:
                    # Extract titles
                    titles = [video['title'] for video in videos if video.get('title')]
                    print(f"âœ… ê²€ìƒ‰ìœ¼ë¡œ {len(titles)}ê°œ ì˜ìƒ ì œëª© ìˆ˜ì§‘ ì™„ë£Œ")

                    # Extract keywords from titles
                    keywords = self._extract_keywords_from_titles(titles, num_keywords)

                    if keywords:
                        print(f"âœ… ê²€ìƒ‰ ê¸°ë°˜ {len(keywords)}ê°œ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ")
                        return keywords

        except Exception as e:
            print(f"âš ï¸  ê²€ìƒ‰ API ì˜¤ë¥˜: {e}")

        # Ultimate fallback: use templates
        print(f"âš ï¸  í…œí”Œë¦¿ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± ì¤‘...")
        templates = {
            "ê²Œì„": [
                "ì¸ê¸° ê²Œì„ ë¦¬ë·°", "ê²Œì„ ê³µëµ ê°€ì´ë“œ", "ê²Œì„ í•˜ì´ë¼ì´íŠ¸ ëª¨ìŒ",
                "ì‹ ì‘ ê²Œì„ í”Œë ˆì´", "ê²Œì„ íŒê³¼ ìš”ë ¹", "ê²Œì„ ì—…ë°ì´íŠ¸ ì†Œì‹",
                "ê²Œì„ ëª…ì¥ë©´ ëª¨ìŒ", "ê²Œì„ ë¦¬ë·° ì¶”ì²œ", "ì¸ê¸° ê²Œì„ ìˆœìœ„",
                "ê²Œì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì†¡"
            ],
            "ìŠ¤í¬ì¸ ": [
                "ìŠ¤í¬ì¸  í•˜ì´ë¼ì´íŠ¸ ëª¨ìŒ", "ê²½ê¸° ëª…ì¥ë©´ ë¶„ì„", "ì„ ìˆ˜ ì¸í„°ë·° ëª¨ìŒ",
                "ìŠ¤í¬ì¸  ë‰´ìŠ¤ ì†ë³´", "ê²½ê¸° ë¦¬ë·° ë¶„ì„", "ìŠ¤í¬ì¸  í›ˆë ¨ ì˜ìƒ",
                "ëª…ê²½ê¸° ë‹¤ì‹œë³´ê¸°", "ìŠ¤í¬ì¸  í•´ì„¤ ë°©ì†¡", "ì„ ìˆ˜ ê¸°ëŸ‰ ë¶„ì„",
                "ìŠ¤í¬ì¸  ë§¤ê±°ì§„ ë¦¬ë·°"
            ],
            "ìŒì•…": [
                "ì‹ ê³¡ ë®¤ì§ë¹„ë””ì˜¤ ëª¨ìŒ", "ì¸ê¸° ìŒì•… ì°¨íŠ¸", "ë¼ì´ë¸Œ ê³µì—° ì˜ìƒ",
                "ìŒì•… ì»¤ë²„ ëª¨ìŒ", "ê°€ìˆ˜ ë¬´ëŒ€ ì˜ìƒ", "ì½˜ì„œíŠ¸ ì‹¤í™© ì¤‘ê³„",
                "ìŒì•… ë°©ì†¡ ì¶œì—°", "ì‹ ê³¡ ë°œë§¤ ì†Œì‹", "ìŒì•… ë¦¬ë·° í‰ê°€",
                "íˆíŠ¸ê³¡ ë©”ë“¤ë¦¬ ëª¨ìŒ"
            ],
            "ì˜í™”/ì• ë‹ˆë©”ì´ì…˜": [
                "ì˜í™” ì˜ˆê³ í¸ ëª¨ìŒ", "ì• ë‹ˆë©”ì´ì…˜ ë¦¬ë·°", "ì˜í™” ëª…ì¥ë©´ ë¶„ì„",
                "ì˜í™” í•´ì„ ì˜ìƒ", "ì• ë‹ˆë©”ì´ì…˜ ì¶”ì²œ", "ì˜í™” ì¤„ê±°ë¦¬ ìš”ì•½",
                "ì˜í™” ì—”ë”© í•´ì„", "ì‹œë¦¬ì¦ˆ ì´ì •ë¦¬", "ì• ë‹ˆë©”ì´ì…˜ ëª…ì¥ë©´",
                "ì˜í™” ë¹„í‰ ë¦¬ë·°"
            ],
            "êµìœ¡": [
                "ì‰¬ìš´ êµìœ¡ ê°•ì˜", "ì´ˆë³´ìë¥¼ ìœ„í•œ ì„¤ëª…", "ê¸°ì´ˆë¶€í„° ë°°ìš°ëŠ” ê°€ì´ë“œ",
                "ì‹¤ì „ í™œìš© ê°•ì˜", "ë‹¨ê³„ë³„ í•™ìŠµ ë°©ë²•", "í•µì‹¬ ê°œë… ì •ë¦¬",
                "ì…ë¬¸ì ê°•ì˜ ì¶”ì²œ", "ê³ ê¸‰ ì‹¬í™” í•™ìŠµ", "ì‹¤ìŠµ ê°•ì˜ ëª¨ìŒ",
                "í•µì‹¬ ìš”ì•½ ì •ë¦¬"
            ],
            "ê³¼í•™/ê¸°ìˆ ": [
                "ìµœì‹  ê¸°ìˆ  ë¦¬ë·°", "ê³¼í•™ ì›ë¦¬ ì„¤ëª…", "ê¸°ìˆ  ì‘ë™ ì›ë¦¬",
                "ì œí’ˆ ë¹„êµ ë¶„ì„", "ì‹ ê¸°ìˆ  ì†Œê°œ ì˜ìƒ", "ê³¼í•™ ì‹¤í—˜ ì˜ìƒ",
                "ê¸°ìˆ  ë‰´ìŠ¤ ì†ë³´", "ì œí’ˆ ê°œë´‰ ë¦¬ë·°", "ê³¼í•™ ë‹¤íë©˜í„°ë¦¬",
                "ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„"
            ],
            "ë…¸í•˜ìš°/ìŠ¤íƒ€ì¼": [
                "íŒ¨ì…˜ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ", "ë·°í‹° ë©”ì´í¬ì—… íŒ", "DIY ë§Œë“¤ê¸° ì˜ìƒ",
                "ì¸í…Œë¦¬ì–´ ê¾¸ë¯¸ê¸°", "ìš”ë¦¬ ë ˆì‹œí”¼ ëª¨ìŒ", "ìƒí™œ ê¿€íŒ ì •ë¦¬",
                "ìŠ¤íƒ€ì¼ë§ ë…¸í•˜ìš°", "ì·¨ë¯¸ ë°°ìš°ê¸° ê°•ì¢Œ", "ì‹¤ìš© ì •ë³´ ëª¨ìŒ",
                "ì „ë¬¸ê°€ íŒ ê³µìœ "
            ],
            "ë‰´ìŠ¤/ì •ì¹˜": [
                "ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ì†ë³´", "ì •ì¹˜ ì´ìŠˆ ë¶„ì„", "ì‹œì‚¬ í† ë¡  ì˜ìƒ",
                "ë‰´ìŠ¤ í•´ì„¤ ë°©ì†¡", "ì •ì¹˜ ë‰´ìŠ¤ ì •ë¦¬", "ì‚¬íšŒ ì´ìŠˆ ë¦¬ë·°",
                "êµ­ì œ ë‰´ìŠ¤ ì†ë³´", "ì •ì±… ë¶„ì„ ì˜ìƒ", "ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ëª¨ìŒ",
                "í˜„ì•ˆ ì´ìŠˆ ì •ë¦¬"
            ],
            "ë¹„ì˜ë¦¬/ì‚¬íšŒìš´ë™": [
                "ì‚¬íšŒ ê³µí—Œ í™œë™", "ë´‰ì‚¬ í™œë™ ì˜ìƒ", "ìº í˜ì¸ í™ë³´ ì˜ìƒ",
                "í™˜ê²½ ë³´í˜¸ í™œë™", "ê¸°ë¶€ ë¬¸í™” ì†Œê°œ", "ì‚¬íšŒ ìš´ë™ í˜„ì¥",
                "ìì„  í–‰ì‚¬ ì˜ìƒ", "ê³µìµ ê´‘ê³  ëª¨ìŒ", "ë‚˜ëˆ” ë¬¸í™” ì‹¤ì²œ",
                "ì‚¬íšŒ ë³€í™” ìš´ë™"
            ],
            "ì• ì™„ë™ë¬¼/ë™ë¬¼": [
                "ë°˜ë ¤ë™ë¬¼ ì¼ìƒ ë¸Œì´ë¡œê·¸", "ê·€ì—¬ìš´ ë™ë¬¼ ì˜ìƒ", "ë™ë¬¼ í›ˆë ¨ ê°€ì´ë“œ",
                "ë°˜ë ¤ë™ë¬¼ í‚¤ìš°ê¸° íŒ", "ë™ë¬¼ ë‹¤íë©˜í„°ë¦¬", "ë™ë¬¼ í–‰ë™ ë¶„ì„",
                "í« ì¼€ì–´ ì •ë³´", "ë™ë¬¼ ë³‘ì› ì •ë³´", "ë°˜ë ¤ë™ë¬¼ ìš©í’ˆ ë¦¬ë·°",
                "ë™ë¬¼ ë†€ì´ ì˜ìƒ"
            ],
            "ì—”í„°í…Œì¸ë¨¼íŠ¸": [
                "ì˜ˆëŠ¥ í”„ë¡œê·¸ë¨ ëª¨ìŒ", "ì¸ê¸° ë°©ì†¡ í´ë¦½", "ì—°ì˜ˆì¸ ì¸í„°ë·°",
                "ì˜ˆëŠ¥ ëª…ì¥ë©´ ëª¨ìŒ", "í† í¬ì‡¼ ì˜ìƒ", "ë²„ë¼ì´ì–´í‹° ì‡¼",
                "ì½”ë¯¸ë”” ì˜ìƒ ëª¨ìŒ", "ë¦¬ì–¼ë¦¬í‹° ì‡¼", "ë°©ì†¡ ë¹„í•˜ì¸ë“œ",
                "ì˜ˆëŠ¥ í•˜ì´ë¼ì´íŠ¸"
            ],
            "ì—¬í–‰/ì´ë²¤íŠ¸": [
                "ì—¬í–‰ì§€ ì¶”ì²œ ì˜ìƒ", "ì—¬í–‰ ë¸Œì´ë¡œê·¸", "ì´ë²¤íŠ¸ í˜„ì¥ ì˜ìƒ",
                "ê´€ê´‘ì§€ ì†Œê°œ", "ì—¬í–‰ ì •ë³´ ê°€ì´ë“œ", "ì¶•ì œ í˜„ì¥ ë¦¬í¬íŠ¸",
                "í•´ì™¸ ì—¬í–‰ íŒ", "êµ­ë‚´ ì—¬í–‰ì§€ ì¶”ì²œ", "ì—¬í–‰ ê²½ë¹„ ì ˆì•½ë²•",
                "ì´ë²¤íŠ¸ ì°¸ì—¬ í›„ê¸°"
            ]
        }

        # Get templates for this category
        category_templates = templates.get(
            category,
            [
                "ì¸ê¸° ì˜ìƒ ëª¨ìŒ", "ì¶”ì²œ ì½˜í…ì¸ ", "ë² ìŠ¤íŠ¸ ì˜ìƒ ìˆœìœ„",
                "í•«í•œ ì˜ìƒ ëª¨ìŒ", "ì¬ë¯¸ìˆëŠ” ì˜ìƒ", "í™”ì œì˜ ì˜ìƒ",
                "ë†“ì¹˜ë©´ ì•ˆë˜ëŠ” ì˜ìƒ", "ì¸ê¸° ìˆœìœ„ ëª¨ìŒ", "íŠ¸ë Œë“œ ì˜ìƒ",
                "í™”ì œì˜ ì½˜í…ì¸ "
            ]
        )

        # Return as many templates as requested
        keywords = category_templates[:num_keywords]

        # If we need more, repeat with variations
        while len(keywords) < num_keywords:
            for template in category_templates:
                if len(keywords) >= num_keywords:
                    break
                keywords.append(template)

        return keywords[:num_keywords]

    def explore_category_with_translations(
        self,
        category: str,
        num_keywords: int = 20
    ) -> List[Dict[str, any]]:
        """
        Get trending keywords for a category with multi-language translations

        Returns:
            List of dictionaries with keyword and translations:
            [
                {
                    'keyword': 'ê²Œì„ í•˜ì´ë¼ì´íŠ¸',
                    'translations': {
                        'í•œêµ­ì–´': 'ê²Œì„ í•˜ì´ë¼ì´íŠ¸',
                        'ì˜ì–´': 'game highlights',
                        ...
                    }
                },
                ...
            ]
        """
        # Get trending keywords
        keywords = self.get_trending_keywords(category, num_keywords)

        # Translate each keyword
        results = []
        for keyword in keywords:
            translations = self.translator.translate_to_all_languages(keyword)
            results.append({
                'keyword': keyword,
                'translations': translations
            })

        return results
